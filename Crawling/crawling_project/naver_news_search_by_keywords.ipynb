{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 네이버 키워드로 뉴스 수집하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "query = \"무역전쟁\"\n",
    "params = {\n",
    "    'where': 'news',\n",
    "    'query': query\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(NAVER_SEARCH_URL, params=params)\n",
    "resp.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## soup만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.content, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_contents = soup.find('ul', class_='type01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ul_contents.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ul = []\n",
    "for content in ul_contents.contents:\n",
    "    if not str(content).strip():\n",
    "        continue\n",
    "    new_ul.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_ul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_list = []\n",
    "\n",
    "for li in new_ul:\n",
    "    news_dict = {}\n",
    "    a_tag = li.find('dl').find('dt').find('a')\n",
    "#     print(a_tag)\n",
    "    \n",
    "    news_dict['link'] = a_tag['href']\n",
    "    \n",
    "    detail_resp = requests.get(news_dict['link'])\n",
    "    detail_soup = BeautifulSoup(detail_resp.content, 'html.parser')\n",
    "    \n",
    "    \n",
    "    news_dict['title'] = a_tag.text\n",
    "    \n",
    "#     print(news_dict)\n",
    "    news_list.append(news_dict)\n",
    "    \n",
    "print(news_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(news_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실습예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  10 페이지 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAVER_SEARCH_URL = \"https://search.naver.com/search.naver\"\n",
    "query = \"무역전쟁\"\n",
    "\n",
    "news_list = []\n",
    "\n",
    "for i in range(1, 11):\n",
    "    start_var = 1 \n",
    "    params = {\n",
    "        'where': 'news',\n",
    "        'query': query,\n",
    "        'start': (i*10) + 1\n",
    "    }\n",
    "    # 검색 결과를 가져온다. (page 별로)\n",
    "    resp = requests.get(NAVER_SEARCH_URL, params=params)\n",
    "    # 검색 결과 soup\n",
    "    soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "    \n",
    "    # ul ' ' 제거.\n",
    "    ul_contents = soup.find('ul', class_='type01')\n",
    "    new_ul = []\n",
    "    for content in ul_contents.contents:\n",
    "        if not str(content).strip():\n",
    "            continue\n",
    "        new_ul.append(content)\n",
    "    \n",
    "    # list 탐색\n",
    "    for li in new_ul:\n",
    "        news_dict = {}\n",
    "        a_tag = li.find('dl').find('dt').find('a')\n",
    "    \n",
    "        news_dict['link'] = a_tag['href']\n",
    "        news_dict['title'] = a_tag.text\n",
    "        \n",
    "        print(news_dict['link'], \"에 들어갑니다.\")\n",
    "        # 디테일 페이지 링크를 들어갑니다.\n",
    "        detail_resp = requests.get(news_dict['link'])\n",
    "        detail_soup = BeautifulSoup(detail_resp.content, \n",
    "                                    'html.parser')\n",
    "        news_dict['body'] = detail_soup.find('body')\n",
    "        \n",
    "        news_list.append(news_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행순서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. url을 파악한다.\n",
    "2. url에 접속한다.\n",
    "3. html 파일을 soup로 만든다.\n",
    "4. soup에서 각 항목들을 정리한다.\n",
    "5. soup에서 상세페이지의 href를 가져온다.\n",
    "6. 각 상세페이지의 href를 requests.get()한다.\n",
    "7. soup로 만들고 상세페이지의 각 항목들을 정리한다.\n",
    "8. 반복문을 통해서 pagination을 이동한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 뉴스토픽 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개발순서\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://search.naver.com/search.naver?&where=news&query=%EB%AC%B4%EC%97%AD%EC%A0%84%EC%9F%81&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=0&ds=&de=&docid=&nso=so:r,p:all,a:all&mynews=0&cluster_rank=26&start=11&refresh_start=0')\n",
    "soup = BeautifulSoup(resp.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topic = soup.find('div', id=\"nxfr_htp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_topic.find_all('li'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_topic = soup.find('ol', class_=\"lst_realtime_srch _tab_area\")\n",
    "news_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 방법 3개 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. contents 쓰기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_tags = news_topic.contents\n",
    "new_list = []\n",
    "for t in li_tags:\n",
    "    title = t.find('span', class_=\"tit\").text\n",
    "    new_list.append(title)\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. siblings 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_tag = news_topic.li\n",
    "\n",
    "new_list2 = []\n",
    "\n",
    "title = li_tag.find('span', class_=\"tit\").text\n",
    "new_list2.append(title)\n",
    "\n",
    "for li in li_tag.find_next_siblings('li'):\n",
    "    title = li.find('span', class_=\"tit\").text\n",
    "    new_list2.append(title)\n",
    "    \n",
    "new_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. find_all(recursive=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li_list = news_topic.find_all('li', recursive=False)\n",
    "len(li_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list3 = []\n",
    "for li in li_list:\n",
    "    title = li.find('span', class_=\"tit\").text\n",
    "    new_list3.append(title)\n",
    "    \n",
    "new_list3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp =requests.get(\"https://blog.naver.com/pokara61/221545070553\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(id=\"mainFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
